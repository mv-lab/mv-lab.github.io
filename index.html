<!DOCTYPE HTML>
<html lang="en">

  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Marcos V. Conde</title>
    <meta name="description" content="Ph.D. Doctoral Researcher in Artificial Intelligence (AI) doing science at Sony PlayStation. Senior Data Scientist and Kaggle Grandmaster at H2O.AI.">
    <meta name="author" content="Marcos V. Conde">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <!-- image -->
    <meta property="og:image"  content="images/marcosv.jpg">
    <meta name="twitter:image" content="images/marcosv.jpg">

    <style>

    .pill-container {
      display: flex;
      flex-wrap: wrap;
      justify-content: flex-start;
      margin: 10px;
    }

    .pill {
      border: 1px solid #ddd;
      padding: 10px;
      cursor: pointer;
      transition: background-color 0.3s;
      text-align: center;
      width: 195px;
      height: 210px;
      box-sizing: border-box;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: flex-start;
      margin: 10px;
    }

    .pill:hover {
      background-color: #f1f1f1;
    }

    .thumbnail {
      width: 160px;
      height: 120px;
      object-fit: cover;
      transition: filter 0.3s;
    }


  </style>
    
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <!--BIO-->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Marcos V. Conde
                </p>

                <p>I'm a Ph.D. Candidate in Artificial Intelligence and Computer Vision at the University of W√ºrzburg, advised by <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ">Prof. Radu Timofte</a>. 
                  Part of my work is supported by <a href="https://sonyinteractive.com/en/news/blog/the-visual-computing-group-vcg-is-happy-to-present-the-1st-ais-vision-graphics-and-ai-for-streaming-workshop-at-cvpr-2024/">Sony PlayStation</a> where I'm a Computer Vision Scientist working on Super-Resolution and AI Graphics Enhancement (like NVIDIA DLSS). 
                  <br>
                  I'm also Kaggle Grandmaster at <a href="https://h2o.ai/">H2O.ai</a> working on LLMs and having fun.
                </p>

                
                <p>During 2020/21, I worked at <em>Huawei Noah‚Äôs Ark Lab</em> (London), and received the best intern award for my work on <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19926">camera ISPs</a> supervised by <a href="http://perezpellitero.github.io/">Dr. Eduardo P√©rez-Pellitero</a>.
                </p>

                <p>I received my M.Sc. in Computer Vision from the Autonomous University of Barcelona (UAB) with honours for my work on <a href="https://openaccess.thecvf.com/content/WACV2023/html/Conde_Perceptual_Image_Enhancement_for_Smartphone_Real-Time_Applications_WACV_2023_paper.html"> <em>Real-time Photography Enhancement</em> </a>
                  advised by <a href="https://scholar.google.com/citations?user=gjnuPMoAAAAJ">Javier Vazquez-Corral</a> and <a href="https://scholar.google.com/citations?user=Gv1QGSMAAAAJ">Michael S. Brown</a>.
                <br>
                <br>
                I lead a team of students at <a href="https://cidaut.ai/">CIDAUT AI</a> (Spain) working on cameras for robots!<br>
                (Siempre apoyamos a estudiantes interesados en resolver problemas reales con IA)
                <br>
                </p>

                <p style="text-align:center">
                  <a href="https://scholar.google.com/citations?hl=en&user=NtB1kjYAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mv-lab">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://www.kaggle.com/jesucristo">Kaggle</a> &nbsp;/&nbsp;
                  <a href="https://medium.com/@drmarcosv/about">Medium</a> &nbsp;/&nbsp;
                  <a href="https://huggingface.co/marcosv">Hugging Face ü§ó </a> &nbsp;/&nbsp;
                  <a href="mailto:marcos.conde[at]uni-wuerzburg.de">Email</a>
                </p>

                <p style="margin-top:0.5em;margin-bottom:0em;">
                  <br>
                  <code style="font-size: 90%;">‰∫ï„ÅÆ‰∏≠„ÅÆËõôÂ§ßÊµ∑„ÇíÁü•„Çâ„Åö &nbsp;|&nbsp ‰∫ïÈºÉ‰∏çÂèØ‰ª•Ë™ûÊñºÊµ∑ËÄÖÔºåÊãòÊñºËôõ‰πü </code>
                  <br>
                  <code style="font-size: 90%;">"I learned very early the difference between knowing the name of something and knowing something."
                    - Richard Feynman</code>
                </p>

              </td>

              <td style="padding:2%;width:40%;max-width:40%">
                <img src="images/marcosv.jpg" alt="Marcos V. Conde" class="profile-pic">
                <!--
                <img style="width:85%;max-width:85%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/marcosv.jpg" class="hoverZoomLink">
                -->
              </td>
            </tr>

          </tbody></table>

          <!--NEWS-->

          <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:0 auto;">
            <tbody>
              <tr>
                <td style="padding:20px;vertical-align:top;">
                  <h2>News</h2>
                  <!-- Wrap only the news paragraphs in a scrollable div -->
                  <div class="news-scroll">

                    <p>
                      <code>[July 2025]</code> New workshop! ü§ñüì∑ <a href="https://supercamerai.github.io">Smart Cameras for Smarter Autonomous Robots</a>, at BMVC 2025
                    </p>

                    <p>
                      <code>[July 2025]</code> I host <a href="https://mv-lab.github.io/neuralisp-iccv25/">A Tour Through AI-powered Photography and Imaging</a>, Tutorial at ICCV 2025 üì∏üå¥üåä
                    </p>

                    <p>
                      <code>[July 2025]</code> üì∑ 2 papers accepted at ICCV 2025: <a href="https://arxiv.org/abs/2503.16067">"Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures"</a> and "<b>PixTalk</b>: Controlling Photorealistic Image Processing and Editing with Language"
                    </p>
                    
                    <p>
                      <code>[May 2025]</code> I organize the <a href="https://ai4streaming-workshop.github.io/">2nd Workshop on <b>AI</b> for Content <b>Gen</b>eration, Quality <b>En</b>hancement and <b>S</b>treaming</a> at ICCV 2025 üå¥üåÖüåä
                    </p>

                    <p>
                      <code>[May 2025]</code> I co-org the <a href="https://www.cvlai.net/aim/2025/">AIM 2025 Workshop</a> at ICCV 2025 üå¥üåÖüåä <br>
                      My challenges: (1) Efficient Real-world Deblurring, (2) Real-world RAW Denoising, (3) Perceptual Super-Resolution.
                    </p>
                    
                    <p>
                      <code>[March 2025]</code> CVPR 2025 <a href="https://arxiv.org/abs/2412.13443">DarkIR: Robust Low-Light Image Restoration</a> (x3 accepts) -- 1st paper of my students
                    </p>

                    <p>
                      <code>[Jan 2025]</code> I co-org the <a href="https://www.cvlai.net/ntire/2025/">NTIRE 2025 Workshop</a> at CVPR 2025. My challenges (1) RAW Image Restoration and Super-Resolution, (2) RAW Reconstruction from sRGB.
                    </p>

                    <p>
                      <code>[Oct 2024]</code> ECCV 2024 <a href="https://mv-lab.github.io/InstructIR/"> <b>InstructIR: High-Quality Image Restoration Following Human Instructions</b> </a> is trending on HF ü§ó. 
                      Read the <a href="https://arxiv.org/abs/2401.16468">preprint </a> |  
                      Watch the demo <a href="images/instructir.mp4"> Video </a> |  
                      Our demo is available <a href="https://huggingface.co/spaces/marcosv/InstructIR">Try it now! (click)</a>
                    </p>
      
                    <p>
                      <code>[June 2024]</code> We organize the 1st <a href="https://ai4streaming-workshop.github.io/">AI for Streaming Workshop at CVPR 2024</a>. Check the awesome speakers and challenges in the website. The workshop is sponsored by Sony PlayStation, Meta and Netflix. 
                    </p>
      
                    <p>
                      <code>[Jan 2024]</code> I co-organize the <a href="https://cvlai.net/ntire/2024/">New Trends in Image Restoration and Enhancement (NTIRE)</a> Workshop at CVPR 2024.<br>
                      I organize the RAW SR and Portrait IQA Challenges.
                    </p>
                  
                  </div>
                </td>
              </tr>
            </tbody>
          </table>


        <!-- HIGHLIGHTS -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <h2>Highlights</h2>
            </td>
          </tr>
        </tbody>
        </table>

        <div class="pill-container">
        <div class="pill">
          <a href="https://mv-lab.github.io/neuralisp-iccv25/"><img class="thumbnail" src="neuralisp-iccv25/imgs/banner.png" alt="neuralISP-ICCV25"></a>
          <p><a href="https://mv-lab.github.io/neuralisp-iccv25/"><papertitle>A Tour Through AI-powered Photography and Imaging</papertitle></a><br>ICCV 2025 Tutorial</p>
        </div>

        <div class="pill">
          <a href="https://ai4streaming-workshop.github.io/"><img class="thumbnail" src="images/ais_cvpr24.jpg" alt="ais"></a>
          <p><a href="https://ai4streaming-workshop.github.io/"><papertitle>AI for Content Generation, Quality Enhancement and Streaming</papertitle></a> Workshop</p>
        </div>

        <div class="pill">
          <a href="https://github.com/mv-lab/InstructIR"><img class="thumbnail" src="images/instructir.gif" alt="InstructIR Highlight"></a>
          <p><a href="https://github.com/mv-lab/InstructIR"><papertitle>Restore Images Following Human Instructions (InstructIR)</papertitle></a> ECCV 2024</p>
        </div>

        <div class="pill">
          <a href="https://sonyinteractive.com/en/news/blog/the-visual-computing-group-vcg-is-happy-to-present-the-1st-ais-vision-graphics-and-ai-for-streaming-workshop-at-cvpr-2024/"><img class="thumbnail" src="images/sony_cvpr24.jpeg" alt="sony"></a>
          <p><a href="https://sonyinteractive.com/en/news/blog/the-visual-computing-group-vcg-is-happy-to-present-the-1st-ais-vision-graphics-and-ai-for-streaming-workshop-at-cvpr-2024/">
            <papertitle>Me at Sony PlayStation</papertitle></a><br>AI Graphics Enhancement & Super-Resolution</p>
        </div>

        </div>
        
          <!-- RESEARCH-->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My current research interests include deep learning, low-level computer vision, vision-language models, and computational photography.
                  <br>
                  <em><b>My research aims to achieve real AI Visual Intelligence and Super-human perception.</b> I replace the eyes with cameras and the brain with a computer.</em>
                  <br>
                  Representative papers are <span class="highlight">highlighted</span> -- check my <a href="https://scholar.google.com/citations?user=NtB1kjYAAAAJ&hl=en">Google Scholar</a> to see the complete list.
                </p>

                <p>
                  <b>People:</b> <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ">Radu Timofte</a> (Áà∏Áà∏) &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Gv1QGSMAAAAJ">Michael S. Brown</a> (York University, VP Samsung) &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=jjF4cMYAAAAJ">Tom Bishop</a> (CEO, Glass Imaging) &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=gjnuPMoAAAAJ">Javier Vazquez-Corral</a> (CVC, UAB) &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=BM6XOmUAAAAJ">Sira Ferradans</a> (DXOMARK) &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=XgljlGUAAAAJ">Ioannis Katsavounidis</a> (Meta) &nbsp;/&nbsp;
                  <a href="https://ai.sony/people/Daisuke-Iso/">Daisuke Iso</a> (Sony AI)
                </p>

              </td>
            </tr>
          </tbody></table>

          <!--PAPERS-->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	
    
    <!--PixTalk: Controlling Photorealistic Image Processing and Editing with Language-->
    <tr onmouseout="pixtalk_stop()" onmouseover="pixtalk_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='dummy_image'>
          
          <img src='images/pixtalk.gif' width="170">
          
          </div>

          <img src='images/pixtalk.gif' width="170" >
        </div>
        
        <script type="text/javascript">
          function pixtalk_start() {
            document.getElementById('dummy_image').style.opacity = "1";
          }

          function pixtalk_stop() {
            document.getElementById('dummy_image').style.opacity = "0";
          }
          pixtalk_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
			<span class="papertitle">PixTalk: Controlling Photorealistic Image Processing and Editing with Language</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>, Zihao Lu, Radu Timofte
        <br>
        <em>International Conference on Computer Vision (ICCV) </em>, 2025
        <br>
        Paper, Code and Models upcoming
        <p></p>
        <p>
        We propose the first approach that introduces language and explicit control into the image processing and editing pipeline. PixTalk is a vision-language multi-task image processing model, guided using text instructions. Our method is able to perform the most popular techniques in photography.
        </p>
      </td>
    </tr>	


    <!--Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures-->
    <tr onmouseout="bokeiccv_stop()" onmouseover="bokeiccv_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bokehiccv_image'>
          
          <img src='images/bokeh.gif' width="170">
          
          </div>

          <img src='images/bokeh.gif' width="170" >
        </div>
        
        <script type="text/javascript">
          function bokeiccv_start() {
            document.getElementById('bokehiccv_image').style.opacity = "1";
          }

          function bokeiccv_stop() {
            document.getElementById('bokehiccv_image').style.opacity = "0";
          }
          bokeiccv_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2503.16067">
			<span class="papertitle">Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures</span>
        </a>
        <br>
        Tim Seizinger, Florin-Alexandru Vasluianu, <strong>Marcos V. Conde</strong>, Zongwei Wu, Radu Timofte
        <br>
        <em>International Conference on Computer Vision (ICCV) </em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2503.16067">arXiv</a>, Code and Models upcoming
        <p></p>
        <p>
          The best Bokeh rendering method using neural networks and a novel dataset. The neural model allows to control the apertur from f/16 to f/1.8
        </p>
      </td>
    </tr>	
    
    <!--Extreme Compression of Adaptive Neural Images-->
    <tr onmouseout="ani_stop()" onmouseover="ani_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ani_image'>
          
          <img src='images/kodim15.png' width="170">
          
          </div>

          <img src='images/kodim15_compress.png' width="170" >
        </div>
        
        <script type="text/javascript">
          function ani_start() {
            document.getElementById('ani_image').style.opacity = "1";
          }

          function ani_stop() {
            document.getElementById('ani_image').style.opacity = "0";
          }
          ani_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2405.16807">
			<span class="papertitle">Extreme Compression of Adaptive Neural Images</span>
        </a>
        <br>
        Leo Hoshikawa*, <strong>Marcos V. Conde*</strong>, Takeshi Ohashi, Atsushi Irie
        <br>
        <em>International Conference on Computer Vision (ICCV) Workshop</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2405.16807">arXiv</a>
        <p></p>
        <p>
          We present a novel analysis on compressing neural fields, with the focus on images. We also introduce Adaptive Neural Images (ANI), an efficient neural representation that enables adaptation to different inference or transmission requirements.
        </p>
      </td>
    </tr>	

    <!--DarkIR-->
    <tr onmouseout="darkir_stop()" onmouseover="darkir_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='darkir_image'>
          
          <img src='images/darkir_out.webp' width="170">
          <!--
          <video  width="170" height="150" muted autoplay loop>
          <source src="images/instructir.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
            -->
          
          </div>

          <img src='images/darkir_in.png' width="170" >
        </div>
        
        <script type="text/javascript">
          function darkir_start() {
            document.getElementById('darkir_image').style.opacity = "1";
          }

          function darkir_stop() {
            document.getElementById('darkir_image').style.opacity = "0";
          }
          darkir_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2412.13443">
      <span class="papertitle">DarkIR: Robust Low-Light Image Restoration</span>
        </a>
        <br>
        Daniel Feijoo, Juan C. Benito, Alvaro Garcia, <strong>Marcos V. Conde</strong>
        <br>
        <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025
        <br>
        <a href="https://github.com/cidautai/DarkIR">GitHub</a>
        /
        <a href="https://arxiv.org/abs/2412.13443">arXiv</a>
        /
        <a href="https://huggingface.co/spaces/Cidaut/DarkIR">Demo ü§ó</a>

        <p></p>
        <p>
          <em><b>DarkIR</b> In low-light conditions, you have noise and blur in the images, yet, previous methods cannot tackle dark noisy images and dark blurry using a single model. 
            We propose the first approach for all-in-one low-light restoration including illumination, noisy and blur enhancement.
        </p>
      </td>
    </tr>

    <!--InstructIR-->
    <tr onmouseout="inst_stop()" onmouseover="inst_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='inst_image'>
          
          <img src='images/instructir.gif' width="170">
          <!--
          <video  width="170" height="150" muted autoplay loop>
          <source src="images/instructir.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
           -->
          
          </div>

          <img src='images/instructir.png' width="170" >
        </div>
        
        <script type="text/javascript">
          function inst_start() {
            document.getElementById('inst_image').style.opacity = "1";
          }

          function inst_stop() {
            document.getElementById('inst_image').style.opacity = "0";
          }
          inst_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://mv-lab.github.io/InstructIR/">
			<span class="papertitle">InstructIR: High-Quality Image Restoration Following Human Instructions</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?user=uIlyqRwAAAAJ&hl=en">Gregor Geigle</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>European Conference on Computer Vision (ECCV)</em>, 2024
        <br>
        <a href="https://mv-lab.github.io/InstructIR/">project page</a>
        /
        <a href="https://github.com/mv-lab/InstructIR/">GitHub</a>
        /
        <a href="https://arxiv.org/abs/2401.16468">arXiv</a>
        /
        <a href="images/instructir.mp4"> <b>Video</b> </a>
        /
        <a href="https://twitter.com/Gradio/status/1752776176811041049">Twitter X</a>
        /
        <a href="https://huggingface.co/spaces/marcosv/InstructIR">Demo ü§ó</a>

        <p></p>
        <p>
          <em><b>InstructIR</b> takes as input a degraded image and a human-written instruction for how to improve that image.</em>
          The (single) neural model performs all-in-one image restoration. We achieve state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement.
        </p>
      </td>
    </tr>        

    <!--Streaming Neural Images-->
    <tr onmouseout="sni_stop()" onmouseover="sni_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='sni_image'>
          
          <img src='images/sni.png' width="170">
          
          </div>

          <img src='images/sni_noise.png' width="170" >
        </div>
        
        <script type="text/javascript">
          function sni_start() {
            document.getElementById('sni_image').style.opacity = "1";
          }

          function sni_stop() {
            document.getElementById('sni_image').style.opacity = "0";
          }
          sni_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2409.17134">
			<span class="papertitle">Streaming Neural Images</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>, Andy Bigos,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>IEEE International Conference on Image Processing (ICIP)</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2409.17134">arXiv</a>
        <p></p>
        <p>
          Implicit Neural Representations (INRs) are a novel paradigm for signal representation that have attracted considerable interest for image compression. We explore the critical yet overlooked limiting factors of INRs, such as computational cost, unstable performance, and robustness
        </p>
      </td>
    </tr>

    <!--RAWIR-->
    <tr onmouseout="rawir_stop()" onmouseover="rawir_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='rawir_image'>
          
          <img src='images/rawir_after.png' width="170">
          
          </div>

          <img src='images/rawir_before.png' width="170" >
        </div>
        
        <script type="text/javascript">
          function rawir_start() {
            document.getElementById('rawir_image').style.opacity = "1";
          }

          function rawir_stop() {
            document.getElementById('rawir_image').style.opacity = "0";
          }
          rawir_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2409.18204">
      <span class="papertitle">Toward Efficient Deep Blind RAW Image Restoration</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>, Florin Vasluianu,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>IEEE International Conference on Image Processing (ICIP)</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2409.18204">arXiv</a>
        <p></p>
        <p>
          We tackle image restoration directly in the RAW domain (yes, it is tricky and a bit crazy).
        </p>
      </td>
    </tr>

    <!--SimpleISP-->
    <tr onmouseout="sisp_stop()" onmouseover="sisp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='sisp_image'>
          
          <img src='images/sisp/20201031-144714_3_6.jpg' width="170">
          
          </div>

          <img src='images/sisp/20201031-144714_3_6_raw.jpg' width="170" >
        </div>
        
        <script type="text/javascript">
          function sisp_start() {
            document.getElementById('sisp_image').style.opacity = "1";
          }

          function sisp_stop() {
            document.getElementById('sisp_image').style.opacity = "0";
          }
          sisp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2404.11569">
			<span class="papertitle">Simple Image Signal Processing using Global Context Guidance</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=8v3dYzEAAAAJ&hl=en">Omar Elezabi</a>,
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>IEEE International Conference on Image Processing (ICIP)</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2404.11569">arXiv</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        <p></p>
        <p>
          First, we propose a novel module that can be integrated into any neural ISP to capture the global context information from the full RAW images. 
          Second, we propose an efficient and simple neural ISP that utilizes our proposed module.
        </p>
      </td>
    </tr>

    <!--NILUT-->
    <tr onmouseout="nilut_stop()" onmouseover="nilut_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nilut_image'>
            
            <img src='images/nilut/001_LUT01.jpg' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/nilut/001.jpg' width="170">
        </div>
        <script type="text/javascript">
          function nilut_start() {
            document.getElementById('nilut_image').style.opacity = "1";
          }

          function nilut_stop() {
            document.getElementById('nilut_image').style.opacity = "0";
          }
          nilut_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/mv-lab/nilut">
          <span class="papertitle">NILUT: Conditional Neural Implicit 3D Lookup Tables for Image Enhancement</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?user=gjnuPMoAAAAJ">Javier Vazquez-Corral</a> ,
        <a href="https://scholar.google.com/citations?user=Gv1QGSMAAAAJ">Michael S. Brown</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>AAAI</em>, 2024 &nbsp <font color="red"><strong>(23.75% Acceptance rate)</strong></font>
        <br>
        <a href="https://mv-lab.github.io/nilut/">project page</a>
        /
        <a href="https://arxiv.org/abs/2306.11920">arXiv</a>
        /
        <a href="https://github.com/mv-lab/nilut">GitHub & Demo</a>
        /
        <a href="images/nilut/nilut.gif">Video</a>
        /
        <a href="images/michael_nilut.webp">Easter Egg</a>
        <p></p>
        <p>
          NILUTs are neural representations of real 3D LUTs for controllable photo-realistic image enhancement and color manipulation.
          Moreover, a NILUT can be extended to incorporate multiple styles into a single network with the ability to blend styles implicitly. 
        </p>
      </td>
    </tr>

    <!--RTSR 2024-->
    <tr onmouseout="piq_stop()" onmouseover="piq_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/rtsr24.png' width="170" >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2404.16484">
			<span class="papertitle">Real-Time 4K Super-Resolution of Compressed AVIF Images. AIS 2024 Challenge Survey</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        Zhijun Lei, Wen Li, Cosmin Stejerean, Ioannis Katsavounidis,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>CVPR Workshop</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2404.16484">arXiv</a>
        /
        <a href="https://github.com/eduardzamfir/ntire23-rtsr">GitHub</a>
        <p></p>
        <p>
          Collaboration with Meta. This paper introduces a novel benchmark as part of the AIS 2024 Real-Time Image Super-Resolution (RTSR) Challenge, which aims to upscale compressed images from 540p to 4K resolution (4x factor) in real-time on commercial GPUs.
          The images are compressed using the modern AVIF codec, instead of JPEG.
        </p>
      </td>
    </tr>

    <!--PIQ DXO-->
    <tr onmouseout="piq_stop()" onmouseover="piq_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/piq24.png' width="170" >
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2404.11159">
			<span class="papertitle">Deep Portrait Quality Assessment. A NTIRE 2024 Challenge Survey</span>
        </a>
        <br>
        Nicolas Chahine,
        <strong>Marcos V. Conde</strong>,
        Daniela Carfora, Gabriel Pacianotto, Benoit Pochon, Sira Ferradans,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>CVPR Workshop</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2404.11159">arXiv</a>
        /
        <a href="https://github.com/dxomark-research/piq2023">GitHub</a>
        <p></p>
        <p>
          Collaboration with DXOMARK. This paper reviews the NTIRE 2024 Portrait Quality Assessment Challenge, highlighting the proposed solutions and results. 
          This challenge aims to obtain an efficient deep neural network capable of estimating the perceptual quality of real portrait photos.
        </p>
      </td>
    </tr>

    <!--BSRAW Challenge-->
    <tr onmouseout="bsrawc_stop()" onmouseover="bsrawc_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bsrawc_image'>
            
            <img src='images/bsraw/gt_crop_190.png' width="170" height="150">

          </div>
          <img src='images/bsraw/in_crop_190.png' width="170" height="150">
        </div>
        <script type="text/javascript">
          function bsrawc_start() {
            document.getElementById('bsrawc_image').style.opacity = "1";
          }

          function bsrawc_stop() {
            document.getElementById('bsrawc_image').style.opacity = "0";
          }
          bsrawc_stop()
        </script>
      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/mv-lab/AISP">
          <span class="papertitle">Deep RAW Image Super-Resolution. A NTIRE 2024 Challenge Survey</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=es&user=kHHzuyoAAAAJ">Florin Vasluianu</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>CVPR Workshop</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2404.16223">arXiv</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        <p></p>
        <p>New methods for RAW Super-Resolution could be essential in modern Image Signal Processing (ISP) pipelines, however, this problem is not as explored as in the RGB domain. 
          The goal of this challenge is to upscale RAW Bayer images by 2x, considering unknown degradations such as noise and blur.
        </p>
      </td>
    </tr>

    <!--BSRAW-->
    <tr onmouseout="bsraw_stop()" onmouseover="bsraw_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bsraw_image'>
            
            <img src='images/bsraw/gt_crop_177.png' width="170" height="150">

          </div>
          <img src='images/bsraw/in_crop_177.png' width="170" height="150">
        </div>
        <script type="text/javascript">
          function bsraw_start() {
            document.getElementById('bsraw_image').style.opacity = "1";
          }

          function bsraw_stop() {
            document.getElementById('bsraw_image').style.opacity = "0";
          }
          bsraw_stop()
        </script>
      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/mv-lab/AISP">
          <span class="papertitle">BSRAW: Improving Blind RAW Image Super-Resolution</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=es&user=kHHzuyoAAAAJ">Florin Vasluianu</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>WACV</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2312.15487">arXiv</a>
        /
        <a href="https://openaccess.thecvf.com/content/WACV2024/html/Conde_BSRAW_Improving_Blind_RAW_Image_Super-Resolution_WACV_2024_paper.html">cvf Proceedings</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        <p></p>
        <p>We advance RAW sensor images up-scaling (Super-Resolution). We explore diverse image degradations (e.g. Noise, Blur) to emulate a low-resolution RAW image, and we train a neural network to upsample it.
        </p>
      </td>
    </tr>

    <!--BOKEH 2023-->
    <tr onmouseout="bokeh_stop()" onmouseover="bokeh_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <div class="one">
          <div class="two" id='bokeh_image'>
            
            <img src='images/bokeh_after.jpg' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/bokeh_before.jpg' width="170">
        </div>
        <script type="text/javascript">
          function bokeh_start() {
            document.getElementById('bokeh_image').style.opacity = "1";
          }

          function bokeh_stop() {
            document.getElementById('bokeh_image').style.opacity = "0";
          }
          bokeh_stop()
        </script>

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Seizinger_Efficient_Multi-Lens_Bokeh_Effect_Rendering_and_Transformation_CVPRW_2023_paper.pdf">
          <span class="papertitle">
            Efficient multi-lens bokeh effect rendering and transformation</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?hl=en&user=PKdW78wAAAAJ">Tim Seizinger*</a>,
        <strong>Marcos V. Conde*</strong>,
        <a href="https://scholar.google.com/citations?hl=en&user=lZDp1vQAAAAJ">Manuel Kolmet</a>,
        <a href="https://scholar.google.com/citations?user=jjF4cMYAAAAJ&hl=en">Tom E Bishop</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>CVPR Workshop</em>, 2023
        <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Seizinger_Efficient_Multi-Lens_Bokeh_Effect_Rendering_and_Transformation_CVPRW_2023_paper.pdf">paper</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Seizinger_Efficient_Multi-Lens_Bokeh_Effect_Rendering_and_Transformation_CVPRW_2023_paper.html">cvf Proceedings</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        <p></p>
        <p>EBokehNet, an efficient state-of-the-art solution for Bokeh effect transformation and rendering. We can render Bokeh from all-in-focus images, 
          or transform the Bokeh of one lens to the effect of another lens without harming the sharp foreground in the image.
        </p>
      </td>
    </tr>

    <!--RTSR 2023-->
    <tr onmouseout="rtsr_stop()" onmouseover="rtsr_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <div class="one">
          <div class="two" id='rtsr_image'>
            
            <img src='images/rtsr_after.jpg' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/rtsr_before.jpg' width="170">
        </div>
        <script type="text/javascript">
          function rtsr_start() {
            document.getElementById('rtsr_image').style.opacity = "1";
          }

          function rtsr_stop() {
            document.getElementById('rtsr_image').style.opacity = "0";
          }
          rtsr_stop()
        </script>

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://eduardzamfir.github.io/NTIRE23-RTSR/">
          <span class="papertitle">
            Towards Real-Time 4K Image Super-Resolution</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=5-FIWKoAAAAJ&hl=en">Eduard Zamfir</a>,
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>CVPR Workshop</em>, 2023
        <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zamfir_Towards_Real-Time_4K_Image_Super-Resolution_CVPRW_2023_paper.pdf">paper</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Zamfir_Towards_Real-Time_4K_Image_Super-Resolution_CVPRW_2023_paper.html">cvf Proceedings</a>
        /
        <a href="https://github.com/eduardzamfir/NTIRE23-RTSR">GitHub</a>
        <p></p>
        <p>The paper presents an exhaustive study of baseline methods for real-time image SR. The methods allow 60 FPS and even 120 FPS.
        </p>
      </td>
    </tr>

    <!--RTSR 2023-->
    <tr onmouseout="rtsrc_stop()" onmouseover="rtsrc_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <div class="one">
          <img src='images/super-res.png' width="170">
        </div>

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://eduardzamfir.github.io/NTIRE23-RTSR/">
          <span class="papertitle">
            Efficient Deep Models for Real-Time 4K Image Super-Resolution. NTIRE 2023 Benchmark and Report</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?user=5-FIWKoAAAAJ&hl=en">Eduard Zamfir</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>CVPR Workshop</em>, 2023
        <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Conde_Efficient_Deep_Models_for_Real-Time_4K_Image_Super-Resolution._NTIRE_2023_CVPRW_2023_paper.pdf">paper</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Conde_Efficient_Deep_Models_for_Real-Time_4K_Image_Super-Resolution._NTIRE_2023_CVPRW_2023_paper.pdf">cvf Proceedings</a>
        /
        <a href="https://github.com/eduardzamfir/NTIRE23-RTSR">GitHub</a>
        <p></p>
        <p>The paper gauges the state-of-the-art methods for real-time 4K upscaling 
          using our new dataset and benchmark protocol. Ovr 100 participants joined the challenge and 15 teams proposed diverse solutions.
          The methods allow 60 FPS and even 120 FPS on regular GPUs.
        </p>
      </td>
    </tr>

    <!--LPIENET-->
    <tr onmouseout="lpienet_stop()" onmouseover="lpienet_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <div class="one">
          <div class="two" id='lpienet_image'>
            
            <img src='images/lpienet_after.png' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/lpienet_before.png' width="170">
        </div>
        <script type="text/javascript">
          function lpienet_start() {
            document.getElementById('lpienet_image').style.opacity = "1";
          }

          function lpienet_stop() {
            document.getElementById('lpienet_image').style.opacity = "0";
          }
          lpienet_stop()
        </script>

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/mv-lab/AISP">
          <span class="papertitle">
            Perceptual Image Enhancement for Smartphone Real-Time Applications</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=es&user=kHHzuyoAAAAJ">Florin Vasluianu</a>,
        <a href="https://scholar.google.com/citations?user=gjnuPMoAAAAJ">Javier Vazquez-Corral</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>WACV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Spotlight)</strong></font>
        <br>
        <a href="https://arxiv.org/abs/2210.13552">arXiv</a>
        /
        <a href="https://openaccess.thecvf.com/content/WACV2023/html/Conde_Perceptual_Image_Enhancement_for_Smartphone_Real-Time_Applications_WACV_2023_paper.html">cvf Proceedings</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        <p></p>
        <p>We propose LPIENet, a lightweight network for perceptual image enhancement, with the focus on deploying it on smartphones.
          The model was tested for image denoising, deblurring, and HDR correction.
        </p>
      </td>
    </tr>

    <!--SWIN2SR-->
    <tr onmouseout="swin2sr_stop()" onmouseover="swin2sr_start()"  bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <div class="one">
          <div class="two" id='swin2sr_image'>
            
            <img src='images/swin2sr_after.png' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/swin2sr_before.jpg' width="170">
        </div>
        <script type="text/javascript">
          function swin2sr_start() {
            document.getElementById('swin2sr_image').style.opacity = "1";
          }

          function swin2sr_stop() {
            document.getElementById('swin2sr_image').style.opacity = "0";
          }
          swin2sr_stop()
        </script>

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2209.11345.pdf">
          <span class="papertitle">
            Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=es&user=MMF5LCoAAAAJ">Ui-Jin Choi</a>,
        <a href="https://scholar.google.com/citations?hl=es&user=7S_l2eAAAAAJ">Maxime Burchi</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>ECCV Workshop </em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2210.13552">arXiv</a>
        /
        <a href="https://link.springer.com/chapter/10.1007/978-3-031-25063-7_42">eccv Proceedings</a>
        /
        <a href="https://github.com/mv-lab/swin2sr">GitHub</a>
        /
        <a href="https://replicate.com/mv-lab/swin2sr">Demo (3M runs!)</a>
        <p></p>
        <p>Super-resolution of compressed images using transformers. 
          We use the Swin Transformer V2, to improve SwinIR for image super-resolution, and in particular, the compressed input scenario.
          Using this method we can tackle the major issues in training transformer vision models, such as training instability, resolution gaps between pre-training and fine-tuning.
        </p>
      </td>
    </tr>

    <!--ReverseISP-->
    <tr onmouseout="risp_stop()" onmouseover="risp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <div class="one">
          <div class="two" id='risp_image'>
            
            <img src='images/risp_raw.jpg' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/risp_rgb.jpg' width="170">
        </div>
        <script type="text/javascript">
          function risp_start() {
            document.getElementById('risp_image').style.opacity = "1";
          }

          function risp_stop() {
            document.getElementById('risp_image').style.opacity = "0";
          }
          risp_stop()
        </script>

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2210.11153.pdf">
          <span class="papertitle">
            Reversed Image Signal Processing and RAW Reconstruction</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>,
        <em> et al. </em>
        <br>
        <em>ECCV Workshop </em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2210.11153">arXiv</a>
        /
        <a href="https://link.springer.com/chapter/10.1007/978-3-031-25066-8_1">eccv Proceedings</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        <p></p>
        <p>
          This paper introduces the AIM 2022 Challenge on Reversed Image Signal Processing and RAW Reconstruction. 
          We aim to recover raw sensor images from the corresponding RGBs without metadata and, by doing this, "reverse" the ISP transformation.
        </p>
      </td>
    </tr>

    <!--Model-Based ISP-->
    <tr onmouseout="misp_stop()" onmouseover="misp_start()"  bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='misp_image'>
            <img src='images/misp/a0016-jmac_MG_0795_rgb.jpg' width="170">
          <!--
          <video  width=100% height=100% muted autoplay loop>
          <source src="images/zipnerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
          -->
          
        </div>
          <img src='images/misp/a0016-jmac_MG_0795_raw.jpg' width="170">
        </div>
        <script type="text/javascript">
          function misp_start() {
            document.getElementById('misp_image').style.opacity = "1";
          }

          function misp_stop() {
            document.getElementById('misp_image').style.opacity = "0";
          }
          misp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/mv-lab/AISP">
          <span class="papertitle">Model-Based Image Signal Processors via Learnable Dictionaries</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>, 
        <a href="https://scholar.google.com/citations?user=k8-q2AoAAAAJ&hl=en">Steven McDonagh</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=Zo97gUQAAAAJ">Matteo Maggioni</a> ,
        <a href="https://www.cs.bham.ac.uk/~leonarda/">Ale≈° Leonardis</a>, 
        <a href="http://perezpellitero.github.io/">Eduardo P√©rez-Pellitero</a>
        <br>
        <em>AAAI</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation, Spotlight)</strong></font>
        <br>
        <a href="https://github.com/mv-lab/AISP">project page</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        /
        <a href="https://arxiv.org/abs/2201.03210">arXiv</a>
        /
        <a href="images/misp/misp.png">Poster</a>
        <p></p>
        <p>
          Hybrid model-based and data-driven approach for modelling ISPs using learnable dictionaries. 
          We explore RAW image reconstruction and improve downstream tasks like RAW Image Denoising via raw data augmentation-synthesis.
        </p>
      </td>
    </tr>

    <!--CLIP Art-->
    <tr onmouseout="clipart_stop()" onmouseover="clipart_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <img src='images/clipart.png' width="170">

        
      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2204.14244">
          <span class="papertitle">CLIP-Art: Contrastive Pre-Training for Fine-Grained Art Classification</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?user=4kdpidQAAAAJ&hl=en">Kerem Turgutlu</a>
        <br>
        <em>CVPR Workshop</em>, 2021
        <br>
        <a href="https://arxiv.org/abs/2204.14244">arXiv</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Conde_CLIP-Art_Contrastive_Pre-Training_for_Fine-Grained_Art_Classification_CVPRW_2021_paper.html">cvpr Proceedings</a>
        /
        <a href="https://github.com/KeremTurgutlu/self_supervised">GitHub</a>
        /
        <a href="https://www.kaggle.com/c/imet-2021-fgvc8">Kaggle</a>
        <p></p>
        <p>
          We were one of the 1st attempts to use CLIP (Contrastive Language-Image Pre-Training) for training a neural network on a variety of art images and descriptions, being able to learn directly from raw descriptions about images, or if available, curated labels.
        </p>
      </td>
    </tr>

    <!-- END PAPERS --->
    </tbody></table>
    <!-- END PAPERS --->

    <!-- SERVICE --->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <h2>Academic Service</h2>

          <p>
            <b> <em>Teaching:</em> </b> Image Processing and Computational Photography (IPCP), Computer Vision (CV)
          </p>


          <p>
            <b> <em>Reviewer:</em> </b> <a href="https://eccv.ecva.net/Conferences/2024/Reviewers">Outstanding Reviewer at ECCV 2024</a>. CVPR 2022-2025, ECCV 2022-24, ICCV 2023-2025, AAAI 2023-24, SIGGRAPH 2024, ACCV 2024, IEEE Transactions on Image Processing, IEEE Transactions on Computational Imaging, IEEE Transactions on Pattern Analysis and Machine Intelligence
          </p>

          <p>
            <b> <em>Workshops:</em> </b> 
            <a href="https://cvlai.net/aim/2025/">AIM 2025 ICCV</a> &nbsp;/&nbsp;
            <a href="https://cvlai.net/ntire/2025/">NTIRE 2025 CVPR</a> &nbsp;/&nbsp; 
            <a href="https://cvlai.net/aim/2024/">AIM 2024 ECCV</a> &nbsp;/&nbsp;
            <a href="https://ai4streaming-workshop.github.io/"> <b>AI for Streaming (AIS 2024) CVPR</b> </a> &nbsp;/&nbsp;
            <a href="https://cvlai.net/ntire/2024/">NTIRE 2024 CVPR</a> &nbsp;/&nbsp; 
            <a href="https://www.vqeg.org/meetings-home/">VQEG 2023</a> &nbsp;/&nbsp; 
            <a href="https://cvlai.net/ntire/2023/">NTIRE 2023 CVPR</a> &nbsp;/&nbsp;
            <a href="https://data.vision.ee.ethz.ch/cvl/aim22/">AIM 2022 ECCV</a>
          </p>

        </td>
      </tr>
    </tbody></table>
    
    <!-- OTHER PROJECTS --->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <h2>Other Projects</h2>
        </td>
      </tr>
    </tbody></table>

    <!--H2O GPT-->
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="https://h2o.ai/"> <img src="images/h2ogpt.gif" width="100%"> </a>
        </td>
        <td width="75%" valign="center">
          <b>H2O Open Ecosystem for LLMs</b>
          <p>
            We introduce a complete open-source ecosystem for developing and testing LLMs. 
            The goal of this project is to boost open alterna- tives to closed-source approaches.
            <br>
            <em>Authors: Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Chun Ming Lee, <b>Marcos V. Conde</b></em>
          </p>

          <a href="https://aclanthology.org/2023.emnlp-demo.6.pdf"> ACL EMNLP Proceedings</a> &nbsp;/&nbsp;
          <a href="https://gpt.h2o.ai/"> Demo</a> &nbsp;/&nbsp;
          <a href="https://github.com/h2oai/h2ogpt">GitHub</a>
        </td>
      </tr>
    </tbody></table>

    <!--Autonomous Driving-->
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/mp-av.gif" width="100%">
        </td>
        <td width="75%" valign="center">
          <b>Motion Prediction for Autonomous Driving</b>
          <p>
            We proposed multiple methods for motion prediction in Autonomous Driving. 
            The methods were presented at CVPR Workshops, ICRA, and IEEE Transactions on Intelligent Transportation Systems (T-ITS, Q1, IF:8.5).
            <br>
            <em>Authors: Carlos G√≥mez-Hu√©lamo, <b>Marcos V. Conde</b></em>
          </p>

          <a href="https://ieeexplore.ieee.org/document/10352999"> IEEE Journal</a> &nbsp;/&nbsp;
          <a href="https://ieeexplore.ieee.org/document/10208487"> CVPR</a> &nbsp;/&nbsp;
          <a href="https://github.com/Cram3r95/mapfe4mp">GitHub</a>
        </td>
      </tr>
    </tbody></table>

    <!--EyeT-->
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2404.11770"> <img src="images/3et_demo_low_res.gif" width="100%"> </a>
        </td>
        <td width="75%" valign="center">
          <b>Event-Based Eye Tracking - AIS Challenge CVPR 2024 </b>
          <p>
            This survey reviews the AIS 2024 Event-Based Eye Tracking (EET) Challenge. The task of the challenge focuses on processing eye movement recorded with event cameras and predicting the pupil center of the eye. 
            <br>
            <em>Authors: Zuowen Wang, Chang Gao, Zongwei Wu, <b>Marcos V. Conde</b>, Radu Timofte, Shih-Chii Liu, Qinyu Chen</em>
          </p>

          <a href="https://arxiv.org/abs/2404.11770">arXiv</a> &nbsp;/&nbsp;
          <a href="https://www.kaggle.com/competitions/event-based-eye-tracking-ais2024"> Kaggle</a> &nbsp;/&nbsp;
          <a href="https://github.com/EETChallenge/challenge_demo_code">GitHub</a> &nbsp;/&nbsp;
          <a href="https://eetchallenge.github.io/EET.github.io/">Project</a>

        </td>
      </tr>
    </tbody></table>


    <!--KUZUSHIJI-->
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="images/kuzushiji.jpg"> <img src="images/kuzushiji.jpg" width="100%"> </a>
        </td>
        <td width="75%" valign="center">
          <b>Kuzushiji Recognition (Nov. 2019)</b>
          <p>
            I was invited by Japan‚Äôs National Institute of Informatics (NII) and ROIS-DS Center for Open Data in the Humanities (CODH) 
            to present a novel solution for the <a href="https://www.kaggle.com/c/kuzushiji-recognition">Kuzushiji Recognition Challenge</a> 
            at the Japanese Culture and AI Symposium 2019 in Tokyo.
            <br>
            <em>Organizers: Dr. Asanobu Kitamoto, Tarin Clanuwat, Alex Lamb.</em>
          </p>

          <a href="https://github.com/mv-lab/kuzushiji-recognition">GitHub</a> &nbsp;/&nbsp;
          <a href="https://www3.nhk.or.jp/news/special/sci_cul/2019/11/story/story_20191120/index.html">NHK News Report</a> &nbsp;/&nbsp;
          <a href="images/tokyo_prize.mp4">Awards Ceremony Video</a>
        </td>
      </tr>
    </tbody></table>

          <!-- FOOTER --->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <br>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
    <footer style="width:100%;text-align:center;padding:18px 0 10px 0;font-size:14px;color:#888;background:transparent;letter-spacing:0.5px;">
      &copy; 2025 Marcos V. Conde. All rights reserved.
    </footer>
</html>
