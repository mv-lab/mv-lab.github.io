<!DOCTYPE HTML>
<html lang="en">

  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Marcos V. Conde</title>
    <meta name="description" content="Ph.D. Doctoral Researcher in Artificial Intelligence (AI) doing science at Sony PlayStation. Senior Data Scientist and Kaggle Grandmaster at H2O.AI.">
    <meta name="author" content="Marcos V. Conde">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <!--BIO-->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Marcos V. Conde
                </p>

                <p>I'm a PhD Researcher in Artificial Intelligence at the University of W√ºrzburg, advised by <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ">Prof. Radu Timofte</a>. 
                  My work is supported by Sony PlayStation (FTG) where I'm a Computer Vision Research Scientist.<br>
                I am also Senior Data Scientist (Kaggle Grandmasters Team) at <a href="https://h2o.ai/">H2O.ai</a>.
                </p>

                <p>From 2020 to 2022, I worked at <em>Huawei Noah‚Äôs Ark Lab</em> (London) supervised by <a href="http://perezpellitero.github.io/">Dr. Eduardo P√©rez-Pellitero</a> and <a href="https://www.cs.bham.ac.uk/~leonarda/">Prof. Ale≈° Leonardis</a>, and received the best intern award for my work on <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19926">neural camera ISPs</a>.
                </p>

                <p>M.Sc. in Computer Vision from the Autonomous University of Barcelona (UAB) with honours for my work on <a href="https://openaccess.thecvf.com/content/WACV2023/html/Conde_Perceptual_Image_Enhancement_for_Smartphone_Real-Time_Applications_WACV_2023_paper.html">Real-time Photography Enhancement</a>.
                <br>
                <br>
                </p>

                <p style="text-align:center">
                  <a href="mailto:marcos.conde@uni-wuerzburg.de">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=NtB1kjYAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mv-lab">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://www.kaggle.com/jesucristo">Kaggle</a> &nbsp;/&nbsp;
                  <a href="https://huggingface.co/marcosv">Hugging Face ü§ó </a>
                </p>

                <p style="margin-top:0.5em;margin-bottom:0em;">
                  <br>
                  <code style="font-size: 90%;">‰∫ï„ÅÆ‰∏≠„ÅÆËõôÂ§ßÊµ∑„ÇíÁü•„Çâ„Åö &nbsp;|&nbsp ‰∫ïÈºÉ‰∏çÂèØ‰ª•Ë™ûÊñºÊµ∑ËÄÖÔºåÊãòÊñºËôõ‰πü </code>
                  <br>
                  <code style="font-size: 90%;">"I learned very early the difference between knowing the name of something and knowing something."
                    - Richard Feynmann</code>
                </p>

              </td>

              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:85%;max-width:85%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/marcosv-scholar.png" class="hoverZoomLink">
              </td>
            </tr>

          </tbody></table>

          <!--NEWS-->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <p>
                <code>[Jan 2024]</code> New paper <a href="https://mv-lab.github.io/InstructIR/"> <b>InstructIR: High-Quality Image Restoration Following Human Instructions</b> </a> is trending on HF ü§ó. 
                Read the <a href="https://arxiv.org/abs/2401.16468">arXiv preprint </a> |  
                Watch the demo <a href="images/instructir.mp4"> Video </a> |  
                Our demo is available <a href="https://huggingface.co/spaces/marcosv/InstructIR">Try it now! (click)</a>
              </p>

              <p>
                <code>[Jan 2024]</code> We organize the 1st <a href="https://ai4streaming-workshop.github.io/">AI for Streaming Workshop at CVPR 2024</a>. Check the awesome speakers and challenges in the website. The workshop is sponsored by Sony PlayStation, Meta and Netflix. 
              </p>

              <p>
                <code>[Jan 2024]</code> I co-organize the <a href="https://cvlai.net/ntire/2024/">New Trends in Image Restoration and Enhancement (NTIRE)</a> Workshop at CVPR 2024.<br>
                I organize the RAW SR, Burst ISP and Portrait IQA Challenges.
              </p>
            </td>
          </tr>
        </tbody></table>

          <!--RESEARCH-->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My current research interests include neural networks, deep learning, low-level computer vision, inverse problems, computational photography and photorealism.
                  <em>How can we improve/change/enhance cameras using AI?</em><br>
                  Representative papers are <span class="highlight">highlighted</span>.
                </p>

                <p>
                  (Past/Present) <b>Collaborators:</b> <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ">Radu Timofte</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Gv1QGSMAAAAJ">Michael S. Brown</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=jjF4cMYAAAAJ">Tom E. Bishop</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=gjnuPMoAAAAJ">Javier Vazquez-Corral</a> 
                </p>

              </td>
            </tr>
          </tbody></table>

          <!--PAPERS-->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	
	
    <!--InstructIR-->
    <tr onmouseout="inst_stop()" onmouseover="inst_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='inst_image'>
          
          <img src='images/instructir.gif' width="170">
          <!--
          <video  width="170" height="150" muted autoplay loop>
          <source src="images/instructir.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
           -->
          
          </div>

          <img src='images/instructir.png' width="170" >
        </div>
        
        <script type="text/javascript">
          function inst_start() {
            document.getElementById('inst_image').style.opacity = "1";
          }

          function inst_stop() {
            document.getElementById('inst_image').style.opacity = "0";
          }
          inst_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://mv-lab.github.io/InstructIR/">
			<span class="papertitle">High-Quality Image Restoration Following Human Instructions</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?user=uIlyqRwAAAAJ&hl=en">Gregor Geigle</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://mv-lab.github.io/InstructIR/">project page</a>
        /
        <a href="https://github.com/mv-lab/InstructIR/">GitHub</a>
        /
        <a href="https://arxiv.org/abs/2401.16468">arXiv</a>
        /
        <a href="images/instructir.mp4"> <b>Video</b> </a>
        /
        <a href="https://twitter.com/Gradio/status/1752776176811041049">Twitter X</a>
        /
        <a href="https://huggingface.co/spaces/marcosv/InstructIR">Demo ü§ó</a>

        <p></p>
        <p>
          <em><b>InstructIR</b> takes as input a degraded image and a human-written instruction for how to improve that image.</em>
          The (single) neural model performs all-in-one image restoration. We achieve state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement.
        </p>
      </td>
    </tr>        

    <!--NILUT-->
    <tr onmouseout="nilut_stop()" onmouseover="nilut_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nilut_image'>
            
            <img src='images/nilut/001_LUT01.jpg' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/nilut/001.jpg' width="170">
        </div>
        <script type="text/javascript">
          function nilut_start() {
            document.getElementById('nilut_image').style.opacity = "1";
          }

          function nilut_stop() {
            document.getElementById('nilut_image').style.opacity = "0";
          }
          nilut_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/mv-lab/nilut">
          <span class="papertitle">NILUT: Conditional Neural Implicit 3D Lookup Tables for Image Enhancement</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?user=gjnuPMoAAAAJ">Javier Vazquez-Corral</a> ,
        <a href="https://scholar.google.com/citations?user=Gv1QGSMAAAAJ">Michael S. Brown</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>AAAI</em>, 2024
        <br>
        <a href="https://mv-lab.github.io/nilut/">project page</a>
        /
        <a href="https://arxiv.org/abs/2306.11920">arXiv</a>
        /
        <a href="https://github.com/mv-lab/nilut">GitHub & Demo</a>
        /
        <a href="images/nilut/nilut.gif">Video</a>
        <p></p>
        <p>
          NILUTs are neural representations of real 3D LUTs for controllable photo-realistic image enhancement and color manipulation.
          Moreover, a NILUT can be extended to incorporate multiple styles into a single network with the ability to blend styles implicitly. 
        </p>
      </td>
    </tr>

    <!--BSRAW-->
    <tr onmouseout="bsraw_stop()" onmouseover="bsraw_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <img src='images/bsraw.png' width="170">

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/mv-lab/AISP">
          <span class="papertitle">BSRAW: Improving Blind RAW Image Super-Resolution</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=es&user=kHHzuyoAAAAJ">Florin Vasluianu</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>WACV</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2312.15487">arXiv</a>
        /
        <a href="https://openaccess.thecvf.com/content/WACV2024/html/Conde_BSRAW_Improving_Blind_RAW_Image_Super-Resolution_WACV_2024_paper.html">cvf Proceedings</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        <p></p>
        <p>We advance RAW sensor images up-scaling (Super-Resolution). We explore diverse image degradations (e.g. Noise, Blur) to emulate a low-resolution RAW image, and we train a neural network to upsample it.
        </p>
      </td>
    </tr>

    <!--LPIENET-->
    <tr onmouseout="lpienet_stop()" onmouseover="lpienet_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <div class="one">
          <div class="two" id='lpienet_image'>
            
            <img src='images/lpienet_after.png' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/lpienet_before.png' width="170">
        </div>
        <script type="text/javascript">
          function lpienet_start() {
            document.getElementById('lpienet_image').style.opacity = "1";
          }

          function lpienet_stop() {
            document.getElementById('lpienet_image').style.opacity = "0";
          }
          lpienet_stop()
        </script>

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/mv-lab/AISP">
          <span class="papertitle">
            Perceptual Image Enhancement for Smartphone Real-Time Applications</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=es&user=kHHzuyoAAAAJ">Florin Vasluianu</a>,
        <a href="https://scholar.google.com/citations?user=gjnuPMoAAAAJ">Javier Vazquez-Corral</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>WACV</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2210.13552">arXiv</a>
        /
        <a href="https://openaccess.thecvf.com/content/WACV2023/html/Conde_Perceptual_Image_Enhancement_for_Smartphone_Real-Time_Applications_WACV_2023_paper.html">cvf Proceedings</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        <p></p>
        <p>We propose LPIENet, a lightweight network for perceptual image enhancement, with the focus on deploying it on smartphones.
          The model was tested for image denoising, deblurring, and HDR correction.
        </p>
      </td>
    </tr>

    <!--SWIN2SR-->
    <tr onmouseout="swin2sr_stop()" onmouseover="swin2sr_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <div class="one">
          <div class="two" id='swin2sr_image'>
            
            <img src='images/swin2sr_after.png' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/swin2sr_before.jpg' width="170">
        </div>
        <script type="text/javascript">
          function swin2sr_start() {
            document.getElementById('swin2sr_image').style.opacity = "1";
          }

          function swin2sr_stop() {
            document.getElementById('swin2sr_image').style.opacity = "0";
          }
          swin2sr_stop()
        </script>

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2209.11345.pdf">
          <span class="papertitle">
            Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=es&user=MMF5LCoAAAAJ">Ui-Jin Choi</a>,
        <a href="https://scholar.google.com/citations?hl=es&user=7S_l2eAAAAAJ">Maxime Burchi</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>
        <br>
        <em>ECCV Workshop </em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2210.13552">arXiv</a>
        /
        <a href="https://link.springer.com/chapter/10.1007/978-3-031-25063-7_42">eccv Proceedings</a>
        /
        <a href="https://github.com/mv-lab/swin2sr">GitHub</a>
        /
        <a href="https://replicate.com/mv-lab/swin2sr">Demo (3M runs!)</a>
        <p></p>
        <p>Super-resolution of compressed images using transformers. 
          We use the Swin Transformer V2, to improve SwinIR for image super-resolution, and in particular, the compressed input scenario.
          Using this method we can tackle the major issues in training transformer vision models, such as training instability, resolution gaps between pre-training and fine-tuning.
        </p>
      </td>
    </tr>

    <!--ReverseISP-->
    <tr onmouseout="risp_stop()" onmouseover="risp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <div class="one">
          <div class="two" id='risp_image'>
            
            <img src='images/risp_raw.jpg' width="170">
          <!--
            <video  width=100% height=100% muted autoplay loop>
            <source src="images/nilut.gif" type="video/gif">
            Your browser does not support the video tag.
            </video>
          -->
          </div>
          <img src='images/risp_rgb.jpg' width="170">
        </div>
        <script type="text/javascript">
          function risp_start() {
            document.getElementById('risp_image').style.opacity = "1";
          }

          function risp_stop() {
            document.getElementById('risp_image').style.opacity = "0";
          }
          risp_stop()
        </script>

      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2210.11153.pdf">
          <span class="papertitle">
            Reversed Image Signal Processing and RAW Reconstruction</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?hl=en&user=u3MwH5kAAAAJ">Radu Timofte</a>,
        <em> et al. </em>
        <br>
        <em>ECCV Workshop </em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2210.11153">arXiv</a>
        /
        <a href="https://link.springer.com/chapter/10.1007/978-3-031-25066-8_1">eccv Proceedings</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        <p></p>
        <p>
          This paper introduces the AIM 2022 Challenge on Reversed Image Signal Processing and RAW Reconstruction. 
          We aim to recover raw sensor images from the corresponding RGBs without metadata and, by doing this, "reverse" the ISP transformation.
        </p>
      </td>
    </tr>

    <!--Model-Based ISP-->
    <tr onmouseout="misp_stop()" onmouseover="misp_start()"  bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='misp_image'>
            <img src='images/misp/a0016-jmac_MG_0795_rgb.jpg' width="170">
          <!--
          <video  width=100% height=100% muted autoplay loop>
          <source src="images/zipnerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video>
          -->
          
        </div>
          <img src='images/misp/a0016-jmac_MG_0795_raw.jpg' width="170">
        </div>
        <script type="text/javascript">
          function misp_start() {
            document.getElementById('misp_image').style.opacity = "1";
          }

          function misp_stop() {
            document.getElementById('misp_image').style.opacity = "0";
          }
          misp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/mv-lab/AISP">
          <span class="papertitle">Model-Based Image Signal Processors via Learnable Dictionaries</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>, 
        <a href="https://scholar.google.com/citations?user=k8-q2AoAAAAJ&hl=en">Steven McDonagh</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=Zo97gUQAAAAJ">Matteo Maggioni</a> ,
        <a href="https://www.cs.bham.ac.uk/~leonarda/">Ale≈° Leonardis</a>, 
        <a href="http://perezpellitero.github.io/">Eduardo P√©rez-Pellitero</a>
        <br>
        <em>AAAI</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation, Spotlight)</strong></font>
        <br>
        <a href="https://github.com/mv-lab/AISP">project page</a>
        /
        <a href="https://github.com/mv-lab/AISP">GitHub</a>
        /
        <a href="https://arxiv.org/abs/2201.03210">arXiv</a>
        /
        <a href="images/misp/misp.png">Poster</a>
        <p></p>
        <p>
          Hybrid model-based and data-driven approach for modelling ISPs using learnable dictionaries. 
          We explore RAW image reconstruction and improve downstream tasks like RAW Image Denoising via raw data augmentation-synthesis.
        </p>
      </td>
    </tr>

    <!--CLIP Art-->
    <tr onmouseout="clipart_stop()" onmouseover="clipart_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">

        <img src='images/clipart.png' width="170">

        
      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2204.14244">
          <span class="papertitle">CLIP-Art: Contrastive Pre-Training for Fine-Grained Art Classification</span>
        </a>
        <br>
        <strong>Marcos V. Conde</strong>,
        <a href="https://scholar.google.com/citations?user=4kdpidQAAAAJ&hl=en">Kerem Turgutlu</a>
        <br>
        <em>CVPR Workshop</em>, 2021
        <br>
        <a href="https://arxiv.org/abs/2204.14244">arXiv</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2021W/CVFAD/html/Conde_CLIP-Art_Contrastive_Pre-Training_for_Fine-Grained_Art_Classification_CVPRW_2021_paper.html">cvpr Proceedings</a>
        /
        <a href="https://github.com/KeremTurgutlu/self_supervised">GitHub</a>
        /
        <a href="https://www.kaggle.com/c/imet-2021-fgvc8">Kaggle</a>
        <p></p>
        <p>
          We were one of the 1st attempts to use CLIP (Contrastive Language-Image Pre-Training) for training a neural network on a variety of art images and descriptions, being able to learn directly from raw descriptions about images, or if available, curated labels.
        </p>
      </td>
    </tr>

    <!-- END PAPERS --->
    </tbody></table>
    <!-- END PAPERS --->

          
        <!-- OTHER PROJECTS --->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Other Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <a href="images/kuzushiji.jpg"> <img src="images/kuzushiji.jpg" width="100%"> </a>
              </td>
              <td width="75%" valign="center">
                <b>Kuzushiji Recognition (Nov. 2019)</b>
                <p>
                  I was invited by Japan‚Äôs National Institute of Informatics (NII) and ROIS-DS Center for Open Data in the Humanities (CODH) 
                  to present a novel solution for the <a href="https://www.kaggle.com/c/kuzushiji-recognition">Kuzushiji Recognition Challenge</a> 
                  at the Japanese Culture and AI Symposium 2019 in Tokyo.
                  <br>
                  <em>Hosts: Dr. Asanobu Kitamoto, Tarin Clanuwat, Alex Lamb.</em>
                </p>

                <a href="https://github.com/mv-lab/kuzushiji-recognition">Solution GitHub</a> &nbsp;/&nbsp;
                <a href="https://www3.nhk.or.jp/news/special/sci_cul/2019/11/story/story_20191120/index.html">NHK News Report</a> &nbsp;/&nbsp;
                <a href="images/tokyo_prize.mp4">Awards Ceremony Video</a>
              </td>
            </tr>
          </tbody></table>

          <!-- FOOTER --->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <br>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
